<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sampling Equations From Variational Autoencoder Embeddings A Novel Approach to Symbolic Regression | Lisa Artmann </title> <meta name="author" content="Lisa Artmann"> <meta name="description" content="Development of new approach to symbolic regression by using principles of Bayesian linear regression in combination with a variational autoencoder to fit non-linear functions additionally."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lartmann.github.io/projects/6_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lisa</span> Artmann </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Sampling Equations From Variational Autoencoder Embeddings A Novel Approach to Symbolic Regression</h1> <p class="post-description">Development of new approach to symbolic regression by using principles of Bayesian linear regression in combination with a variational autoencoder to fit non-linear functions additionally.</p> </header> <article> <blockquote> <h1 id="abstract">Abstract</h1> <p>The contribution investigates a new approach to symbolic regression that uses the principles of Bayesian linear regression to fit nonlinear functions additionally. In the future, this may enable hierarchical modeling that encodes individual differences in latent space and model architecture. To fill this methodological gap, the embedding of a (variational) autoencoder is used to perform a continuous relaxation of the discrete search space of functions and sample equations. Different architectures and model parameters are tested to find those that best satisfy the properties of continuity, semantic ordering of the equations, and searchability. The two main architectures have the difference that one uses n-hot encoded equation elements while the other implements one-hot encoded function terms. Both of which are implemented as standard and variational autoencoder. From the current results, a difference in performance among these two architectures can be observed. In detail, the autoencoders with n-hot encoded equation elements did not satisfy the continuity property as they produced impossible equations and thus resulted in a sparse space. However, the autoencoders with one-hot encoded function terms produced an embedding that satisfied all of the requirements. Finally, it is shown that this approach can work in principle, as it was possible to derive equations from the data using MCMC sampling.</p> </blockquote> <h1 id="introduction">Introduction</h1> <p>Symbolic regression is a subfield of artificial intelligence that aims to derive closed-form mathematical formulae from observed data <a class="citation" href="#eq_discovery_theory">(Todorovski &amp; Dvzeroski, 2001)</a>. This means that it takes the observed data as input and outputs the mathematical formula that best describes these data <a class="citation" href="#sym_reg_1">(Schmidt &amp; Lipson, 2009; Todorovski, 2017)</a>. In this instance, best refers to a well-balanced trade-off between simplicity and accuracy <a class="citation" href="#wang_symbolic_2019">(Wang et al., 2019)</a>.</p> <p>There is the possibility of using trained neural networks as models, but understanding and insight are difficult to achieve. Symbolic regression models can learn longer time horizons compared to neural network models because they derive the underlying principle and are therefore more robust to noise. <a class="citation" href="#sindyc">(Brunton et al., 2016)</a> For these reasons, symbolic regression algorithms are optimal for identifying appropriate mathematical equations that can be further transformed and interpreted to gain insight <a class="citation" href="#sym_reg_2">(Simidjievski et al., 2020)</a>.</p> <p>For scientific purposes, the relationship between the variables is often more interesting than the prediction alone. This is why symbolic regression is already used in physics with symbolic regression algorithms such as AI Feynman <a class="citation" href="#ai_feyman">(Udrescu &amp; Tegmark, 2020)</a>, which uses neural networks to find equations from the field of physics. However, a variety of algorithms has been adopted to discover the most appropriate equations. For example, symbolic regression using genetic programming <a class="citation" href="#SR_symbolic_reg">(Augusto &amp; Barbosa, 2000)</a>, or deep symbolic regression uses a gradient-based approach based on reinforcement learning. <a class="citation" href="#petersen2021deep">(Petersen et al., 2021)</a></p> <p>Despite the success of symbolic regression algorithms in the domain of physics, they have not been widely implemented in the behavioral and brain sciences such as psychology or neuroscience. Human sciences present challenges that are not as present in other domains. One such challenge is the large individual differences between subjects. This means that if each participant contributes multiple data points, then trying to find a linear regression line for all participants at once could violate the assumption of independence since the data are dependent on the individual. For this reason, hierarchical approaches such as Bayesian hierarchical linear regression play a pivotal role in these areas. They allow for modeling at the individual and group level simultaneously and can be seen as a generalization of linear models. <a class="citation" href="#multilevel">(Krull &amp; MacKinnon, 2001; Lee &amp; Webb, 2005)</a></p> <p>The overarching objective of this thesis is to introduce a novel approach to symbolic regression that extends Bayesian linear regression to implement nonlinear equations\footnote{Functions can be understood as a form of equations. Equations referred to in the context of the implementation described in this thesis are also functions as they map an independent variable to a dependent variable.} and can, in the future, be further expanded for Bayesian hierarchical regression.</p> <p>Specifically, this thesis focuses on developing a neural network architecture that creates a continuous space of equations that sampling algorithms can search to identify the most appropriate equation for the given data. This procedure relies on an autoencoder that represents semantic similarity by vector similarity in the embedding, also called latent space. This embedding can transform the discrete search space of function terms into a continuous search space of real numbers. This enables the use of Markov chain Monte Carlo sampling to fit an equation and obtain its underlying probability distribution in the latent space. Consequently, the discrete search space of equations is transformed into a continuous search space by using continuous relaxation. Hence, there are three main properties that the search space needs to fulfill:</p> <ul> <li>Continuity</li> <li>Representation of Semantic Similarity</li> <li>Searchability</li> </ul> <p>Finally, a proof of principle is presented showing that it is indeed possible to search this space and find an equation that describes the observed data.</p> <p>This approach is not limited to neuroscience or psychology and could be adapted for use in various fields of research where Bayesian linear regression is or could be used.</p> <p>The project is implemented using the programming language <code class="language-plaintext highlighter-rouge">Python</code> and mainly its packages <code class="language-plaintext highlighter-rouge">equation_tree</code>, <code class="language-plaintext highlighter-rouge">pytorch</code>, <code class="language-plaintext highlighter-rouge">pandas</code>, <code class="language-plaintext highlighter-rouge">numpy</code>, <code class="language-plaintext highlighter-rouge">seaborn</code>, <code class="language-plaintext highlighter-rouge">matplotlib</code> and <code class="language-plaintext highlighter-rouge">pyro</code>.</p> <h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blr-480.webp 480w,/assets/img/blr-800.webp 800w,/assets/img/blr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Bayesian linear regression model with the dependent variable $y_i$ sampled from a normal distribution where the mean is the point on the regression line and $\sigma^2$ is the variance. In this case, the prior distributions for the parameters are assumed to be normally distributed. </div> <p>Bayesian and frequentist statistics are the two main branches of statistics employed in science. In frequentist statistics, probabilities are seen as a generalization of the facts of random events in repeated execution <a class="citation" href="#bayesian_data">(Gelman et al., 2014)</a>. On the other hand,</p> <blockquote> <p>“[i]n Bayesian statistics, probability is used as the fundamental measure or yardstick of uncertainty”<a class="citation" href="#bayesian_data">(Gelman et al., 2014)</a></p> </blockquote> <p>Put another way, in Bayesian statistics, probabilities can be thought of as the degree of belief or support of a rational agent who is uncertain about the outcome of a random event <a class="citation" href="#degree_of_belief">(Hawthorne, 2005)</a>.</p> <p>The ability to explicitly model uncertainty is considered one of the main advantages of Bayesian statistics over frequentist statistics <a class="citation" href="#bayesian_data">(Gelman et al., 2014; Lynch &amp; Bartlett, 2019)</a>. In addition, this approach takes advantage of prior knowledge about the world to obtain valuable results with a limited amount of data. While frequentist methods are still more common in psychological research, the rate of Bayesian methods is increasing <a class="citation" href="#pychology_bayes">(van de Schoot et al., 2017)</a>.</p> <p>Although there are many different methods in Bayesian statistics, the focus here is on Bayesian linear regression, which is briefly described below.</p> <p>In Bayesian linear regression, the goal is to fit a regression line $\hat{Y}$ described by \(\hat{Y} = \beta_{0} + \beta_{1} X\) to the observed data. Here, $\beta_{0}$ refers to the intercept with the $y$-axis and $\beta_{1}$ refers to the slope of the regression line. From that the formula for a value $y_i$ can be derived: \(\hat{Y}_{i} = \beta_{0} + \beta_{1} X_{i} + \epsilon_{i}\)</p> <p>In this case, $\epsilon_i$ is the individual error of every sample $i$ from the regression line. The goal is to find the posterior distribution for the parameters $\beta_0$ and $\beta_1$, rather than a point estimate.</p> <p>Bayesian hierarchical regression is an approach to Bayesian modeling that can be used when the assumption of independence is not satisfied in the dataset. This can be the case when each participant in an experiment contributes multiple data points. The absolute numbers between participants may be very different while the relationship between the dependent and independent variables may still be the same. Bayesian hierarchical regression can address this problem by modeling each participant at one level and the group at another level.</p> <h2 id="theoretical-framework">Theoretical Framework</h2> <p>This section will go into detail about how the mentioned expansion to nonlinearity in Bayesian regression is implemented and what the requirements are for it to work. Effectively, the nonlinearity is introduced by replacing the parameters $\beta_0$ and $\beta_1$, which are the intercept and slope from the linear regression model, with the $\beta_0$ to $\beta_n$, which are the $n$ latent units from the latent space of the autoencoder.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/stat_model-480.webp 480w,/assets/img/stat_model-800.webp 800w,/assets/img/stat_model-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/stat_model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The proposed model consists of $\mu$, $\sigma^2$, $x_i$, $y_i$, just like the simple linear model. However, the parameters $\beta_0$ and $\beta_1$ are extended to $n$ parameters where $\beta_0, \beta_1, ..., \beta_n$ are the units of the latent space in the autoencoder which have a normal distribution as prior. $y_i$ is then calculated by inserting $x_i$ into the equation derived from the decoder $f_\theta$ when given the latent space units $\beta_0, \beta_1, ..., \beta_n$ as input. </div> <p>As mentioned, this work creates a neural network structure that finds a latent space representation that represents semantically similar equations by their distance in the latent space representation. However, to achieve this, there needs to be a definition of semantic similarity. In linguistics, semantics refers to the meaning that a word provides while syntax refers to the structure of the word <a class="citation" href="#chomsky">(Chomsky, 1955)</a>. In the context of mathematical equations here this idea is borrowed from linguistics which means that semantics refers to the structure of the function graph or the mathematical meaning of the symbols while syntax refers to the structure and building blocks of the function term. In this particular case, semantic similarity is a measure of distance with the values of the function graph being more similar the closer they get. For example, $f(x) = cos(x)$ and $g(x) = sin(x+ \frac{\pi}{2} )$ should have a distance of zero in a perfect latent space representation as they are semantically equal in the mathematical sense because $cos(x) = sin(x+ \frac{\pi}{2})$. There are different ways to measure semantic similarity between two equations. Within the scope of this contribution, I will focus on the meaning of the area between the function graphs which implies that the smaller the area, the higher the similarity. This means that the similarity of any two functions is anti-proportional to the integral of the subtraction of the two functions. \footnote{For the sake of simplicity and because it is not relevant in the practical implementation, the two functions $f$ and $g$ are considered to have no interception with each other.}</p> \[sim(f,g) \propto \frac{1}{ \mid \int f(x) - g(x) dx \mid}\] <p>In the following, the distance function</p> \[d(f,g) = \left| \int f(x) - g(x) dx \right|\] <p>will be used to measure how different two functions $f$ and $g$ are. This difference is here defined to be antiproportional to the similarity.</p> \[sim(f,g) \propto \frac{1}{d(f,g)}\] <p>Therefore, if the distance is minimal, then the similarity is maximal.</p> <p>In practice, this can be approximated using the Manhattan distance of the value vectors times the length of the interval $l$ and divided by the number of data points.</p> \[d(f,g) \approx \left( \sum_i^n \left| f(x_i) - g(x_i)\right| \right) \frac{l}{n}\] <p>Syntax is the second concept that is borrowed from linguistics. Syntactically similar refers to the similarity of the written equation. This means that $2x$ and $-2x$ would be considered syntactically equal as the only syntactic difference is the constant, however, semantically, they are very different since the area between the curves is relatively large, with four in the interval $[-1, 1]$. In this case, two equations with different constants are defined to be syntactically equivalent because the equation is considered in the form where the constant is replaced by a placeholder.</p> <h1 id="methods">Methods</h1> <h2 id="dataset">Dataset</h2> <p>The dataset is generated using the <code class="language-plaintext highlighter-rouge">equation-tree</code> package and every equation in the dataset consists of three parts:</p> <ul> <li>Function Term</li> <li>Constant</li> <li>Values of the Function Graph</li> </ul> <p>Hence, there is a two-dimensional tensor for the function terms with the shape of <code class="language-plaintext highlighter-rouge">(batch size, maximum term length)</code>, a two-dimensional tensor for the constants with the shape <code class="language-plaintext highlighter-rouge">(batch size, 1)</code>, and a three-dimensional tensor for the $x$ and $y$ values of the function graph with the shape <code class="language-plaintext highlighter-rouge">(batch size, 2, 50)</code>.</p> <p>The dataset contains 6000 equations and is split into a train and a test set. The training dataset consists of 80% of the total data and the test dataset contains the leftover data. All the evaluations shown in the result section are created by using the test data, which is not used for training.</p> <h3 id="function-term-in-prefix-notation">Function Term in Prefix Notation</h3> <p>Especially for the autoencoder with n-hot encoded equation symbols, the function terms must be represented as a sequence since the goal is to predict a sequence of mathematical symbols that make up the new function term. Therefore, the prefix notation, also known as the Polish notation, was chosen. This notation converts the tree representation of a function term, where the nodes are composed of elementary functions, operators, variables, and constants, into a sequence.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/eq_tree-480.webp 480w,/assets/img/eq_tree-800.webp 800w,/assets/img/eq_tree-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/eq_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Equation tree for the function term $(1-2) \cdot 3$ which can be transformed to the prefix notation × - 1 2 3. </div> <p>The notation is called prefix because the operator comes before the operands. Compared to the infix notation which is the standard representation of a function term, it does not need parentheses because it is unambiguous. For example, the function term $(1 - 2) \cdot 3$ in infix notation can be converted to the prefix notation <code class="language-plaintext highlighter-rouge">× - 1 2 3</code>.</p> <p>The difference between elementary functions and operators is that an elementary function defines how an equation element $f(x)$ is transformed while an operator defines how two equation elements interact. The function complexity and therefore the number of unique elementary functions is kept simple to avoid redundancy and to enhance learning. It can, however, be easily increased in future applications.</p> <p>The elementary functions used to create the dataset are the trigonometric function $\sin\left(f(x)\right)$ and the exponential function $e^{f(x)}$. As far as the operators are concerned, there are operations like addition, subtraction, and multiplication.</p> <p>For the training, the equations are generated randomly up to a certain tree depth. In the current scenario, the tree depth was chosen to be two. Another restriction to the equations is that there are only equations with a maximum of one dependent variable chosen.</p> <p>The equations and their values are generated by using the \lstinline{equation-tree} python package. The model requires a fixed input shape and therefore the maximum length of a function term in prefix notation is calculated and all other function terms are filled with padding elements to reach the same length. If the maximum length of the function term is six, then padding would have to be added to the function term from above in the following form:</p> <p><code class="language-plaintext highlighter-rouge">× - 1 2 3 &lt;PAD&gt;</code></p> <p>The padding is embedded in the model like any other character.</p> <h3 id="constants">Constants</h3> <p>The constants are separated from the function terms since each constant is a floating point value that is predicted by the model and not a finite set of classes such as the other elements of the mathematical function. Therefore, the prefix function term contains a placeholder for the constant and not its real value. The real value, however, is needed to instantiate the equation and obtain the function graph with its values. Therefore, the real values of the constants are saved separately. The number of constants per equation is set to exactly one to keep a consistent shape of the tensor without any padding.</p> <p>Every equation is 100 times instantiated with different constants. So that there are multiple syntactically equal equations with different constants. This process enables the model to learn how different values for the constants change the function values.</p> <h3 id="function-values">Function Values</h3> <p>The function values are calculated by evaluating every equation 50 times in an equally spaced interval between -1 and 1. These values are later used to calculate the semantic similarity of the equations. Another important aspect here is that there is no gap in the definition, as there would be when dividing by zero, for the equations in the evaluated interval. This is important to avoid complications when calculating the semantic similarity between two equations and because the goal is to derive continuous functions from the data.</p> <h2 id="model-architecture">Model Architecture</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ba_model_classify.drawio-480.webp 480w,/assets/img/ba_model_classify.drawio-800.webp 800w,/assets/img/ba_model_classify.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ba_model_classify.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The autoencoder architecture has two separate encoder networks for the function terms $x_t$ and the constants $x_c$ which are then concatenated and again encoded to the latent space representation $z$. The latent space representation $z$ is then decoded by another dense neural network before it is split into different decoder networks for the reconstructed function terms $\hat{x_t}$ and the reconstructed constants $\hat{x_c}$ </div> <p>There are two types of models used in this approach. Both implement a neural network architecture called an autoencoder, but the second model is a variation of this architecture that implements variational inference and is called a variational autoencoder.</p> <p>An autoencoder is a neural network structure whose goal is to reconstruct the input data. Therefore, it consists of an encoder structure and a decoder structure. The encoder propagates the input through the network into a, in most cases, lower dimensional space in which the input is embedded. Throughout this thesis, this lower-dimensional space will be referred to as the embedding or the latent space. After encoding the input information into the latent space, the next step is to reconstruct the original representation from the latent space, and this task is achieved by the decoder network. The main advantage of this neural network architecture is that it performs a dimension reduction of the input which can be beneficial in many different applications such as image compression and denoising <a class="citation" href="#lucas_theis_lossy_2017">(Theis et al., 2017; Gondara, 2016)</a>.</p> <p>As illustrated, the encoder $h_\phi(x)$ is comprised of separate encoder networks for the function terms and the constants which are then combined in another encoder network that results in the embedding vector $z$. The dimension of $z$ is variable and the performance of different dimensions is tested in the results section. It is important to note that the function values do not serve as input to the network and are merely used in the loss function to calculate the latent correlation loss. The function term encoder consists of an embedding layer with the size of the vocabulary $vocab$ which is the number of unique symbols used. This embedding layer is then fed into a linear layer with 64 units and rectified linear units (ReLU) activation where $ReLU(x) = max(0,x)$. It outputs a predefined dimension which is eight in this case. Lastly, the input is flattened so that the output shape is $(8 \cdot vocab)$. The encoder for the constants is simply a linear layer that expands the dimension of the constants from one to eight. Afterward, the output of the function term encoder and the output of the constant encoder are concatenated and fed to another encoder network where the input is passed through a two-layer multilayer perceptron of size 56 and 64 with ReLU activation after the first layer which outputs the latent space as a vector with its predefined dimension.</p> <p>The decoder $f_\theta(z)$ then increases the dimension with two linear layers of size 64 and 56 and ReLU activation between them. The output is then split and reshaped to have the shape \lstinline{(batch_size, max_term_length, 8)} for the function terms and \lstinline{(batch_size, 8)} for the constants. The function term decoder then consists of one linear layer of size eight that outputs a tensor of shape \lstinline{(batch_size, max_term_length, vocab_size)} which are the logits for every possible symbol at any position in the function term. To construct the function term, the symbol with the highest value is chosen for every individual position in the function term. Hence, the function term decoder reconstructs each character individually. The decoder for the constants consists only of one linear layer that outputs one unit the value of which is the prediction for the constant.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/VAE.drawio-480.webp 480w,/assets/img/VAE.drawio-800.webp 800w,/assets/img/VAE.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/VAE.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> In the variational autoencoder there are two separate encoder and decoder networks for the function terms $x_t$ and the constants $x_c$ just like in the autoencoder. However, the combined encoder network $h_\theta(x)$ results in a mean vector $\mu$ and a vector for the log variance $\log \sigma^2$ from which the latent space vector $z$ is sampled and decoded in the combined decoder $f_\theta(z)$. </div> <p>The variational autoencoder <a class="citation" href="#kingma_auto-encoding_2013">(Kingma &amp; Welling, 2013)</a> is a special kind of autoencoder that incorporates principles from variational inference instead of directly learning the latent representation. It is a probabilistic model which means that instead of mapping the input to a fixed vector, it is mapped to a multivariate distribution.</p> <p>The implementation of this architecture is very similar to the one of the standard autoencoder with the main difference being in how the latent vector is derived. In this case, the encoder does not directly return $z$ but much rather a mean vector $\mu$ and a log variance vector $\log \sigma^2$. These vectors are then used to sample the latent space $z$. The reparametrization trick is applied because the randomness of the sampling operation prevents the direct application of backpropagation. The reparametrization trick separates the deterministic and stochastic parts of the sampling operation by taking a sample $\epsilon$ from a standard normal distribution and then computing $z$ as a deterministic function where $z = \mu + \sigma \cdot \epsilon$.</p> <p>The variational autoencoder can be seen as an implementation of variational inference which means that it is attempting to find the posterior distribution of $z$ given the input $x$. For the implementation of a variational autoencoder, it is important to find the best network parameters $\theta$ or in other words, to find the maximum likelihood estimate for $\theta$ given the data points $x_1, \ldots, x_n \in \mathbb{R}^D$.</p> \[\begin{equation} \operatorname{ELBO} = \mathbb{E}_{q_\phi(z|x_i)} [ \log p_\theta(x_i|z) ] - D_{KL}(q_\phi(z|x_i) \| p(z)) \end{equation}\] <p>The detailed derivation of this formula can be found in the paper by <a class="citation" href="#kingma_auto-encoding_2013">(Kingma &amp; Welling, 2013)</a>. The surrogate distribution and the optimal parameters can be found by maximizing $ELBO$ given $q$ and $\theta$.</p> \[\begin{equation} \hat{q, \theta}:=\arg \max _{q, \theta} \operatorname{ELBO}(q, \theta) \end{equation}\] <p>In the case of the variational autoencoder, the surrogate distribution is calculated by</p> \[\begin{equation} \begin{aligned} q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) &amp; := \mathcal{N}\left(h_\phi^{(1)}(\boldsymbol{x}), \operatorname{diag}\left(\exp \left(h_\phi^{(2)}(\boldsymbol{x})\right)\right)\right) \\ &amp; = \mathcal{N}\left(\mu, \operatorname{diag}\left(\exp \left(\log \sigma^2 \right)\right)\right) \end{aligned} \end{equation}\] <p>where $h_\phi^{(1)}$ and $h_\phi^{(2)}$ are neural networks that map the input $x$ to a mean $\mu$ and a log-variance $\log \sigma^2$ to get the approximate posterior distribution.</p> <p>It is not possible to directly calculate the ELBO because the calculation would involve integrating over the latent variables $z$. Thus, the ELBO is maximized by stochastic gradient ascent and this is where the reparametrization trick comes into play. The surrogate distribution $q_\phi(z \mid x)$ is therefore split into a deterministic function $g_\phi$ and a random variable $\epsilon \sim \mathcal{D}$ where $\mathcal{D}$ is a normal distribution $\mathcal{N}(0, \sigma)$ with the standard deviation derived from the logarithm of the variance and the deterministic function is calculated by $g_\phi(\epsilon, x) = \mu + \sigma \epsilon $. Thus, the function $g_\phi$ can be used to get $z$ as $z := g_\phi(\epsilon, x)$.</p> \[\begin{equation} \operatorname{ELBO} = \underbrace{\mathbb{E}_{q_\phi(z|x_i)} [ \log p_\theta(x_i|z) ]}_{reconstruction} - \underbrace{D_{KL}(q_\phi(z|x_i) \| p(z))}_{regularization} \end{equation}\] <p>Since both $\theta$ and $\phi$ are differentiable, it is now possible to calculate the gradient that is used for gradient descent.</p> <p>In practice, the KL divergence term can be seen as a regularization term on the reconstruction loss which means that the model tries to reconstruct each $x_i$ while at the same time trying to achieve a standard normal distribution for the equations in the latent space $z$. Therefore, the KL divergence between the prior $p(z)$ and the posterior $q(z \mid x)$ is minimized.</p> <p>For the results to be comparable, the specific variational autoencoder implemented has a very similar architecture compared to the described autoencoder. The function term encoder and the constant encoder have the same number and size of layers compared to the standard autoencoder. The encoder, however, consists of a different set of layers that encode the mean and the logarithm of the variance. These encoder networks both have the same number and size of layers as the standard autoencoder. The decoder network, as well as the function term decoder and the constant decoder, are the same as described for the standard autoencoder.</p> <h2 id="loss">Loss</h2> <p>This problem needs an adaptation of the basic reconstruction loss of an autoencoder. First of all, the reconstruction is split into two parts, the reconstruction of the function term syntax and the reconstruction of the constants. The function term reconstruction uses cross-entropy ($CE$) and for the measure of reconstruction of the constants, a mean squared error ($MSE$) is used. Since the latent space should be semantically ordered, it needs to be regularized. Several approaches share this purpose, one of the most remarkable is contrastive loss <a class="citation" href="#contrastiveLoss">(Chopra et al., 2005)</a>. However, this method, like most of the methods to regularize the latent space, is used mainly for computer vision tasks. Hence, their application in this task is limited. Contrastive loss aims to regularize the latent space in a way that positive samples, meaning data points belonging to the same class, are close to each other in the latent space. However, this problem is a mixture of classification and regression. In that sense, there are potentially as many classes as there are equations since, for the similarity of the equations, both the function term and its constant are considered. Or, to put it in another way, there are only negative samples except for very rare cases where they are semantically equal but syntactically different which is very unlikely considering how the dataset is created. Moreover, the constants can take any real number between zero and ten. Therefore, contrastive learning approaches like contrastive loss <a class="citation" href="#contrastiveLoss">(Chopra et al., 2005)</a>, lifted structured loss <a class="citation" href="#song2015deep">(Song et al., 2015)</a>, and triplet loss <a class="citation" href="#TrippletLoss">(Schroff et al., 2015)</a> cannot be used in combination with this method to fulfill the goal of semantic ordering. A second approach is rank-n-contrast <a class="citation" href="#zha2023rankncontrast">(Zha et al., 2023)</a> and it uses data augmentation to create different values for the same label. While this might work in computer vision tasks, where an image of a dog is still an image of a dog even though minor changes are applied like flipping or cropping the image, it is impossible for the task at hand. The function values deterministically follow from the equation and there is no way of changing the equation values without changing the equation itself. Therefore, this thesis introduces a different loss function called latent correlation loss ($LC$) which is described in detail in the next section.</p> <p>The individual components of the loss are added together to calculate the overall loss. The loss function of the standard autoencoder is therefore calculated by the formula</p> \[\begin{equation} L_{AE} = CE(\hat{x_t}, x_t) + MSE(x_c, \hat{x_c}) + LC(v, z). \label{math:loss1} \end{equation}\] <p>Here, $x_t$ and $x_c$ are the original function terms and constants, while $\hat{x_t}$ and $\hat{x_c}$ are the reconstructed function terms and constants. $v$ are the function values and $z$ are the latent vectors. The mean squared error between the original constants $x_c$ and the reconstructed constants $\hat{x_c}$ is calculated by the formula</p> \[\begin{equation} \operatorname{MSE}(x_c, \hat{x_c})=\frac{1}{n} \sum_{i=1}^n\left(x_{c,i}-\hat{x}_{c,i}\right)^2 . \end{equation}\] <p>where $n$ is the batch size.</p> <p>The loss for the function term reconstruction is calculated by cross-entropy loss because it is a state-of-the-art loss function for multi-class classification tasks <a class="citation" href="#Demirkaya2020ExploringTR">(Demirkaya et al., 2020)</a>. It is calculated using the formula</p> \[\begin{equation} CE(\hat{x_t}, x_t) = \frac{\sum_{n=1}^N l_n}{N}, \quad l_n=-\sum_{c=1}^C w_c \log \frac{\exp \left(\hat{x}_{t, n, c}\right)}{\sum_{i=1}^C \exp \left(\hat{x}_{t, n, i}\right)} x_{t, n, c} \label{ce_loss} \end{equation}\] <p>where $C$ is the number of classes, in this case, the number of unique symbols. $w$ is the weight and $l_n$ is the loss for an individual entry of the batch. This is with a mean reduction where the overall $CE$ loss is the average batch loss.</p> <p>For the variational autoencoder, there needs to be the KL divergence loss as an additional term with a weighting term $w$. The complete calculation for the loss of the variational autoencoder is therefore given by $L_{VAE}$</p> \[\begin{equation} L_{VAE} = CE(\hat{x_t}, x_t) + MSE(x_c, \hat{x_c}) + LC(v, z) +w KL(\mu, \log(\sigma^2)) \label{math:loss_vae} \end{equation}\] <p>The KL divergence loss is based on the KL divergence between the prior and the surrogate distribution.</p> \[\begin{equation} \begin{aligned} KL\left(\mu, \log\left(\sigma^2\right)\right) &amp; := D_{KL}\left(q_\phi\left(z|x\right) \| p\left(z\right)\right) \\ &amp; = D_{KL}\left(\mathcal{N}\left(\mu, \operatorname{diag}\left(\exp \left(\log \sigma^2 \right)\right)\right)\| p\left(z\right)\right) \end{aligned} \end{equation}\] <h3 id="latent-correlation-loss">Latent Correlation Loss</h3> <p>The latent correlation loss aims to quantify how well the latent space represents the semantic meaning of the equations.</p> <p>For the sake of simplicity, the loss measures distance, which is defined as the inverse similarity, instead of the similarity. The distance matrix of the equations captures the distance of every equation with every other equation defined by the distance of the function values which approximates the area between the two function curves.</p> \[\begin{equation} d(f_1,f_2) = \left(\sum_i^n \left| f_1(x_i) - f_2(x_i) \right| \right) \frac{l}{n} \end{equation}\] <p>The distance matrix of the function values $F$ for $n$ equations is therefore defined by</p> \[F=\begin{bmatrix} d_{11}&amp;d_{12}&amp;d_{13}&amp;\dots &amp;d_{1n} \\d_{21}&amp;d_{22}&amp;d_{23}&amp;\dots &amp;d_{2n} \\\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots &amp; \\d_{n1}&amp;d_{n2}&amp;d_{n3}&amp;\dots &amp;d_{nn} \\\end{bmatrix}\] <p>The distance of two latent space vectors $w$ and $u$ in a $m$-dimensional latent space can simply be calculated by using the Euclidean distance</p> \[d(\vec{w}, \vec{u}) = \Vert \vec{w} - \vec{u} \Vert = \sqrt{\sum_{i=1...m} (w_{i} - u_{i})^2}\] <p>of the latent vectors. The distance matrix $L$ of the equation distance in the latent space holds the information for the latent space distance of every equation with every other equation.</p> \[L=\begin{bmatrix} d_{11}&amp;d_{12}&amp;d_{13}&amp;\dots &amp;d_{1n} \\d_{21}&amp;d_{22}&amp;d_{23}&amp;\dots &amp;d_{2n} \\\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots &amp; \\d_{n1}&amp;d_{n2}&amp;d_{n3}&amp;\dots &amp;d_{nn} \\\end{bmatrix}\] <p>These two matrices of dimension $n\times n$ would ideally correlate with one. This means that ideally for every change in one matrix, there is a change in the other matrix accordingly. This would mean that the semantic meaning is perfectly represented in the latent space.</p> <p>To calculate the correlation, the distance matrix of the function values and the distance matrix of the latent space are flattened into 1D vectors which means that the dimension of $F$ and $L$ changes from $n\times n$ to $n^2$.</p> <p>The correlation is calculated by the Pearson correlation</p> \[r_{F, L}=\frac{\sum_{i=1}^n\left(F_{i}-\bar{F}\right)\left(L_{i}-\bar{L}\right)}{\sqrt{\sum_{i=1}^n\left(F_{i}-\bar{F}\right)^2 \sum_{i=1}^n\left(L_{i}-\bar{L}\right)^2}}\] <p>where $\bar{F}$ and $\bar{L}$ are the sample mean of $F$ and $L$.</p> <p>This correlation coefficient is then adapted to fit the properties of a loss function. The correlation coefficient can take numbers between zero and one. The latent correlation loss is therefore calculated by</p> \[\begin{equation} LC = - r_{F,L} + 1 \end{equation}\] <p>This formula is chosen because it reverses the relationship of the correlation, meaning that the higher the correlation, the smaller the loss. Furthermore, it makes sure that the loss cannot be negative.</p> <h2 id="training">Training</h2> <p>The chosen optimizer is the stochastic gradient descent algorithm adam <a class="citation" href="#kingma2017adam">(Kingma &amp; Ba, 2017)</a> which is an adaptive learning rate optimization algorithm that is commonly used for the training of deep learning models. The initial learning rate is chosen to be 0.01 but it is adapted by the optimizer to achieve better performance. The other default parameters are a weight of 0.0001 for the KL divergence and a latent dimension of four. Different values for some of these parameters are tested to find the optimal parameter values. Each parametric change of the models is evaluated ten times to get more expressive results as for some metrics there was a lot of variance between different trials.</p> <h2 id="evaluation-metrics">Evaluation Metrics</h2> <p>There are several important metrics to assess the performance of the model. They all measure one of the predefined properties. First, there is the function term reconstruction accuracy, which calculates how many of the function terms are correctly recovered. Secondly, there is the mean squared error between the original constants and the recovered constants. These two metrics test the reconstruction ability to ensure that the latent space embedding of the model is not ambiguous and that one embedding encodes for exactly one function term and constant. Furthermore, it assesses if the decoder can reconstruct the original function term from the input. This also assesses the property of searchability as the space is only searchable if the equations are properly reconstructed otherwise there is no coherence between the semantic ordering that is calculated based on the input during training and the reconstructed equations.</p> <p>Another metric is the correlation coefficient between the distance matrix of the latent space and the distance matrix of the area between the function curves. These two distance matrices should correlate close to one because this would indicate that the semantically similar equations are close to each other in the latent space which is one of the main goals of the embedding. This correlation coefficient is calculated in the same way.</p> <p>To fulfill the property of continuity, each point in the latent space must evaluate to a valid equation. The design of the model allows for impossible equations to occur. This can, for example, be the case when the model predicts padding in the middle of the function term like <code class="language-plaintext highlighter-rouge">['*', '+', 'x', '&lt;PAD&gt;', 'c_1']</code>, would be converted to</p> \[f(x) = (x+&lt;PAD&gt;)*c_1\] <p>which is an equation that cannot be interpreted as the padding has no mathematical value or meaning. There are many more ways that the model can produce mathematically impossible equations. This can be assessed by generating several random embedding vectors and evaluating what percentage of them result in valid equations when decoded by the model.</p> <h1 id="results">Results</h1> <h2 id="performance-per-number-of-latent-dimensions">Performance per Number of Latent Dimensions</h2> <p>The figures show how changing the latent dimensions affects performance. This means that only the size of the latent vector $z$ is changed, and everything else about the model remains the same. The standard autoencoder achieves up to 100% accuracy in function term reconstruction for latent dimensions as small as two. The constant MSE in the standard autoencoder improves up to about four dimensions and then stays between 0.0005 and 0.0010. The mean correlation coefficient of the standard autoencoder improves up to about four dimensions and then stays above 0.99 from then on.</p> <p>The variational autoencoder shows a mean reconstruction accuracy of 99% and higher from just two dimensions on as well. The average constant MSE ranges form 0.007 to 0.008 for latent dimensions of two and higher. It only slightly increases for very large latent dimensions of 64 and 128 where it takes values of 0.013 and 0.015. The average latent correlation of the variational autoencoder increases with the number of latent dimension up until four and five dimensions where it has the highest values with 97% accuracy. Other than the standard autoencoder, it starts to decrease again from then on which is very visible for the dimension of 128 where the average correlation drops to 89%.</p> <h2 id="comparison-of-standard-and-variational-autoencoder">Comparison of Standard and Variational Autoencoder</h2> <p>The standard autoencoder performs slightly better than the variational autoencoder in the correlation between latent and value distance with an average of 0.95 across all dimensions compared to 0.92 in the variational autoencoder. The standard autoencoder is also better in terms of constant MSE, where it achieves an average MSE of 0.025 compared to 0.038 for the variational autoencoder. With regard to the average function term reconstruction accuracy, they have a comparable performance with both times 92%.</p> <p>Therefore, as for the model with n-hot encoded equation elements, the standard autoencoder performs slightly better regarding the evaluation metrics.<br> Nevertheless, the latent space of the variational autoencoder is denser and more regularized.</p> <div class="row"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/latent_space_ae_classify-480.webp 480w,/assets/img/latent_space_ae_classify-800.webp 800w,/assets/img/latent_space_ae_classify-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/latent_space_ae_classify.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vae_latent_space_classify-480.webp 480w,/assets/img/vae_latent_space_classify-800.webp 800w,/assets/img/vae_latent_space_classify-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vae_latent_space_classify.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> This graph shows the first two dimensions of a four-dimensional latent space for both, the standard autoencoder (left) and the variational autoencoder (right). Each point is the embedding of an encoded equation. The Variational autoencoder results in a more dense embedding. </div> <h2 id="weighting-factor-for-kullback-leibner-divergence">Weighting Factor for Kullback Leibner Divergence</h2> <p>This section examines how the weighting for the KL divergence affects the performance and the other losses. The KL divergence loss is required for the regularization of the latent space to be close to a distribution, which in this case is a standard normal distribution. The performance of the variational autoencoder depends strongly on the weight assigned to the Kullback-Leibner divergence. The correlation between the latent and the value distance decreases strongly with a KL divergence weight greater than 0.0001. The accuracy of the function term reconstruction shows a strong decrease between a weight of 0.001 and 0.01, with an extremely low accuracy of less than 8% for weights greater than 0.001. Coherently, the constant MSE rises strongly for weights greater than 0.01. A weight of zero has the best performance across all evaluation metrics.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kl_weights-480.webp 480w,/assets/img/kl_weights-800.webp 800w,/assets/img/kl_weights-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kl_weights.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of different weighting factors for the KL divergence loss. The latent correlation decreases with an increase in weighting. The meaning of the box plots is as described in figure caption. </div> <p>The KL divergence has a negative effect on the other losses. This is illustrated in the figure above, where it can be seen that the constant loss, the reconstruction loss as well as the latent correlation loss increase with an increase in the KL divergence weight.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kl_losses-480.webp 480w,/assets/img/kl_losses-800.webp 800w,/assets/img/kl_losses-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kl_losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The different loss functions (left) and the KL divergence loss (right) for different weighting factors of the Kullback Leibner divergence loss. </div> <p>Even a weighting as low as $10^{-5}$ results in a notable reduction in the KL loss relative to the KL loss when the KL divergence is not considered. Values of 0.0001 and below offer a substantial decline in the KL loss without compromising the performance of the evaluation metrics.</p> <h2 id="latent-correlation-weighting">Latent Correlation Weighting</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/correlation_weighting-480.webp 480w,/assets/img/correlation_weighting-800.webp 800w,/assets/img/correlation_weighting-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/correlation_weighting.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Performance with different weighting factors for the latent correlation loss in the standard autoencoder (AE) and the variational autoencoder (VAE). The interpretation of the box plot is as previously described. </div> <p>This section examines the impact of varying weighting terms for the latent correlation loss on the performance of evaluation metrics. The results are illustrated in the figure above. It can be observed that as the weight for the latent correlation loss in the standard autoencoder increases, the correlation between value distance and latent distance also increases, reaching a correlation of 1.0. The function term reconstruction accuracy is, with an average accuracy higher than 97%, high across all values for the weighting. Nevertheless, the constant MSE exhibits a pronounced increase with weightings exceeding ten.</p> <p>The function term reconstruction accuracy in the variational autoencoder is high across all values for the latent correlation weighting, with only a slight decrease observed at a weighting of 1000. Additionally, in the case of the variational autoencoder, the latent correlation increases with an increase in the weighting term. Similarly, the constant MSE increases with the weighting, though not to the same extent as in the standard autoencoder. The variational autoencoder still achieves a more than three times lower MSE of 0.03 compared to the standard autoencoder at a latent correlation weighting of 1000. A higher weighting for the latent correlation loss increases the correlation without a strong decrease in the reconstruction accuracy. This way, the model can achieve a correlation of up to 0.99.</p> <h2 id="learning-rate">Learning Rate</h2> <p>Although the learning rate is optimized during training, the initial learning rate appears to be crucial and is therefore examined in this section. The function term reconstruction accuracy is high for the learning rates of 0.0001, 0.001, and 0.01 in both the standard autoencoder and the variational autoencoder. Moreover, performance decreases significantly for learning rates smaller or larger than these values. For the constant MSE, the learning rate of 0.1 with an MSE of 50 on average for the variational autoencoder and an MSE of 10 for the standard autoencoder results in poor performance. However, other values for the learning rate result in low values for the constant MSE. The average latent correlation is the highest for a learning rate of 0.001 in both the standard and the variational autoencoder, with correlations of 0.990 and 0.986, respectively. It is observed that the correlation decreases for learning rates higher or lower than 0.001.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/learning_rate_result-480.webp 480w,/assets/img/learning_rate_result-800.webp 800w,/assets/img/learning_rate_result-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/learning_rate_result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of the performance for different values for the initial learning rate in the standard autoencoder (AE) and the variational autoencoder (VAE). The box plot can be interpreted as previously described. </div> <h2 id="mcmc-sampling-on-the-latent-space">MCMC sampling on the Latent Space</h2> <div class="row"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MCMC_vae_6-480.webp 480w,/assets/img/MCMC_vae_6-800.webp 800w,/assets/img/MCMC_vae_6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/MCMC_vae_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MCMC_vae_dist_6-480.webp 480w,/assets/img/MCMC_vae_dist_6-800.webp 800w,/assets/img/MCMC_vae_dist_6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/MCMC_vae_dist_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of a case where the MCMC algorithm derived the right function term and almost the exact constant. On the left are the sampled and the original equations shown and on the right is the distribution for each unit of the eight-dimensional latent space. </div> <p>As previously stated, the latent space is designed to sample equations from it. This hypothesis is validated by creating a Markov chain Monte Carlo (MCMC) model with the <code class="language-plaintext highlighter-rouge">pyro</code> library, which samples from this space to determine the feasibility of the concept in principle. Indeed, the present findings confirm that the MCMC algorithm can be run on the embedding and that the distribution over the latent variables can be identified. In certain instances, the algorithm is able to identify the correct equation by utilizing the embedding generated by the variational autoencoder, as illustrated in the figure above. Moreover, the algorithm is capable of identifying the distribution over the latent dimension.</p> <p>However, in other instances, the algorithm remains within a local optimum and provides a sub-optimal solution. This is shown in the figure below where the derived equations are similar but not equal to the original equation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MCMC_vae_11-480.webp 480w,/assets/img/MCMC_vae_11-800.webp 800w,/assets/img/MCMC_vae_11-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/MCMC_vae_11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of a sub-optimal solution for MCMC algorithm. It shows the graph of the real function $5x$ and the graphs of the resulting functions $cx-sin(x)$ and $cx-e^x$. </div> <h1 id="discussion">Discussion</h1> <p>The autoencoder with one-hot encoded function terms, other than the autoencoder with n-hot encoded equation elements, creates a space that has all the needed properties. It is continuous as valid equations are guaranteed no matter the value of the latent space vector. It can achieve a high semantic ordering in the latent space, which is shown by the high correlation between latent and value distance. Furthermore, the results provide a proof of concept that shows that the latent space is searchable.</p> <p>For both, the standard and variational autoencoders, latent dimensions as small as four seem to be appropriate because the correlation between latent and value distance increases up to this dimension.</p> <p>For the weighting of the KL loss, 0.0001 appears to be good because it strongly decreases the KL loss without sacrificing performance in the measured evaluation metrics. For the standard autoencoder, a weighting for the latent correlation loss of 10 seems to be suitable because it improves the performance of the latent correlation without a decrease in the reconstruction accuracy. For the variational autoencoder, there could even be a weighting of 100 used as it does not seem to have negative effects on the other evaluation metrics. However, since it does not further improve the correlation between latent and value distance, there seems to be no reason to use 100 over 10 for a four-dimensional latent space. A learning rate of 0.001 seems to be the best option, for both the standard and variational autoencoders, of the different learning rates compared. Especially correlation between latent and value distance decreases with learning rates higher or lower than 0.001.</p> <p>It is shown that MCMC sampling on the latent space is possible. However, further investigation is required to identify the circumstances under which the algorithm can identify the true equation. Nevertheless, in every case, the algorithm identified a solution, which demonstrates the feasibility of this approach.</p> <p>The latent space created by the autoencoder with n-hot encoded equation elements did not fulfill the property of continuity, no matter the configurations tested. It created a sparse space where not every vector evaluates to a mathematically possible equation. Therefore, the model architecture was modified to guarantee the mathematical validity of the equation by forcing the model to choose from all the predefined possible function terms. This architectural adjustment ensures that the autoencoder fulfills the property of continuity by default. The model also performed well on the metric of a semantically ordered search space which was shown by a high correlation between function value distances and latent space distances. Furthermore, the space is searchable which was shown by performing MCMC sampling on the space and finding the correct, or close to correct, equation. Variational autoencoders are better suited for generative tasks (\cite{generative_vae}) and this task here is generative as the model generates equations with new constants that it has not encountered during training. Therefore, the variational autoencoder might produce the more preferable search space. Indeed, the variational autoencoder produced a more dense latent space, however, the performance of the MCMC sampling on the latent spaces created by the different kinds of autoencoders is not compared in this thesis. Further testing needs to be done to reach a conclusion on the topic.</p> <p>Unfortunately, because they employ different datasets and equation complexities, the outcomes of the autoencoder with n-hot encoded equation elements cannot be meaningfully compared to the outcomes of the autoencoder with one-hot encoded function terms. The first strategy also failed to meet the basic prerequisite of continuity. For these reasons, the results of the two methods used in this thesis are not compared here.</p> <p>It should be noted that this approach has certain limitations that can be addressed in the future. The number of unique function terms is quite small in the second method with a tree depth of two. However, the number of classes can easily be increased to allow for more complex function terms. After increasing the tree depth, it might also be worthwhile to increase the number of possible constants and later even the number of variables. Especially with an increased number of unique function terms, it might be advisable to increase the dataset size, which can be easily done since the equations are generated.</p> <p>At this current state, it is important to mention that the model is not explicitly penalized for equation complexity. This means that a simpler equation is not necessarily preferred over a more complex equation with the same error. With increasing equation complexity it might be worthwhile to introduce a mechanism that penalizes complexity. This could be done by sampling on embeddings of models trained with different tree depths and comparing the error and complexity of the resulting equations.</p> <p>Another limitation is the simple structure of the encoder and decoder which only uses dense layers. Further studies could explore this issue further by investigating how different layers, such as transformer layers, can improve performance. These could potentially take better account of the sequential structure of the equations than the linear layers implemented in this thesis. It might also be worthwhile to examine the use of the stress function of multidimensional scaling (\cite{mds}) instead of the latent correlation loss and compare the performances.</p> <p>Multidimensional scaling is a statistical method that is used to scale data points in a lower-dimensional space based on their distances. Its objective is therefore quite similar to the goal of this work. It takes the distance matrix and tries to find the optimal distance persevering map by performing eigenvalue decomposition. In classical MDS (\cite{mds}; \cite{mds_gower}), also known as principal coordinate analysis (PCoA), the squared distance matrix is used to minimize the strain.</p> \[\begin{equation} \operatorname{Strain}_D\left(z_1, z_2, \ldots, z_n\right)=\sqrt{\left(\frac{\sum_{i, j}\left(b_{i j}-z_i^T z_j\right)^2}{\sum_{i, j} b_{i j}^2}\right)} \end{equation}\] <p>Here $z$ is the coordinate in the latent space and $b_{ij}$ are the elements of the matrix $B$ that is the double centering matrix derived from the squared distance matrix and used to perform eigenvalue decomposition. Similarly, the stress function of metric multidimensional scaling could be used.</p> \[\operatorname{Stress}_D\left(z_1, z_2, \ldots, z_n\right)=\sqrt{\sum_{i \neq j=1, \ldots, n}\left(d_{i j}-\left\|z_i-z_j\right\|\right)^2}\] <p>In this case, $d_{ij}$ would be the distance of the function values while $\left|z_i-z_j\right|$ would be the distance in the latent space. It is a bit more restrictive than the latent correlation loss introduced in this thesis as it penalizes the two distances to have the same value instead of possibly a multiple.</p> <p>It is also important to note that this thesis did not investigate the implementation of this method to Bayesian hierarchical regression. Future research should certainly further test how the MCMC sampling performs under different circumstances like, for example, noisy data. In addition, it may be worthwhile to try variational inference as an alternative to MCMC sampling.</p> <p>There are also other approaches that introduce nonlinearity to Bayesian regression. One of which is the Bayesian multivariate adaptive regression spline (MARS) <a class="citation" href="#denison_bayesian_1998">(Denison et al., 1998)</a> which builds on <a class="citation" href="#mars">(Friedman, 1991)</a>. This approach divides the data into intervals that are divided by cut points and performs linear regression between these cut points. Although this is nonlinear, it does not result in a continuous function. Therefore, if the underlying equation is nonlinear, then it can only be approximated by this method. This makes it hard to gain insight from the results of this method. Other approaches expand the linear equation to a polynomial equation like $y = \beta_0 + \beta_1 x + \beta_2 x^2 + … + \beta_n x^n$ and use the same strategy as for Bayesian linear regression to fit the model <a class="citation" href="#bayesianPolynomial">(Schulz et al., 2018)</a>. However, this requires knowledge about the equation and is very limited concerning the possible outcomes of the equation. These methods are, in contrast to the method provided in this thesis, quite restricted concerning the nonlinearity and insight they can provide.</p> <p>As mentioned in the introduction, there are many different symbolic regression algorithms. One popular symbolic regression algorithm is sparse identification of nonlinear dynamical systems (SINDy) developed by <a class="citation" href="#sindy">(Brunton et al., 2016)</a>. It is a data-driven approach that aims to identify governing equations for dynamical systems. The core idea behind SINDy is to find a sparse representation of the dynamics using a library of candidate functions. Other than the method introduced in this thesis, SINDy focuses on dynamical systems and modeling how state variables evolve over time. Furthermore, the amount of candidate functions in the library should be limited as the matrix may become ill-conditioned otherwise. Therefore, the selection of candidate functions in the library needs to be carefully chosen for the specific problem and needs knowledge about the domain. Additionally, it represents the derivative as a linear combination of the candidate functions and is therefore limited in complexity. There are also Bayesian approaches to the SINDy algorithm <a class="citation" href="#bayesian_sindy">(Hirsh et al., 2022)</a>, however, they have similar restrictions as mentioned for the original SINDy algorithm.</p> <p>This work sets the foundation for a new approach to symbolic regression that can expand the possibilities of Bayesian hierarchical regression. This may enable hierarchical models that encode individual differences in latent space distance and model architecture. It also allows researchers to question the linearity of the relationships between independent and dependent variables. In the future, it may be applied to many more fields and enable the modeling of increasingly complex problem sets far beyond computational neuroscience and psychology.</p> <h1 id="conclusion">Conclusion</h1> <p>Recent advances in computing power have enabled data-driven approaches that are transforming many fields. It has enabled the use of complex machine learning and, in particular, deep learning models that can provide a great leap forward for science. Symbolic regression is at the heart of these efforts, and therefore of great importance for discovering and understanding the underlying principles that govern the world and reality at large. It can bridge the gap between deep neural networks and human-understandable models. It allows us to capture complex relationships in the data that are not limited to linearity, while at the same time providing the possibility of insight and understanding. This thesis has introduced a new method to this endeavor that offers some advantages over current symbolic regression algorithms. However, there is still work to be done to realize its full potential and to understand how it compares to other symbolic regression algorithms in terms of performance.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zha2023rankncontrast" class="col-sm-8"> <div class="title">Rank-N-Contrast: Learning Continuous Representations for Regression</div> <div class="author"> Kaiwen Zha, Peng Cao, Jeany Son, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yuzhe Yang, Dina Katabi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="bayesian_sindy" class="col-sm-8"> <div class="title">Sparsifying priors for Bayesian uncertainty quantification in model discovery</div> <div class="author"> Seth M. Hirsh, David A. Barajas-Solano, and J. Nathan Kutz </div> <div class="periodical"> <em>Royal Society Open Science</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rsos.211823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p> We propose a probabilistic model discovery method for identifying ordinary differential equations governing the dynamics of observed multivariate data. Our method is based on the sparse identification of nonlinear dynamics (SINDy) framework, where models are expressed as sparse linear combinations of pre-specified candidate functions. Promoting parsimony through sparsity leads to interpretable models that generalize to unknown data. Instead of targeting point estimates of the SINDy coefficients, we estimate these coefficients via sparse Bayesian inference. The resulting method, uncertainty quantification SINDy (UQ-SINDy), quantifies not only the uncertainty in the values of the SINDy coefficients due to observation errors and limited data, but also the probability of inclusion of each candidate function in the linear combination. UQ-SINDy promotes robustness against observation noise and limited data, interpretability (in terms of model selection and inclusion probabilities) and generalization capacity for out-of-sample forecast. Sparse inference for UQ-SINDy employs Markov chain Monte Carlo, and we explore two sparsifying priors: the spike and slab prior, and the regularized horseshoe prior. UQ-SINDy is shown to discover accurate models in the presence of noise and with orders-of-magnitude less data than current model discovery methods, thus providing a transformative method for real-world applications which have limited data. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="petersen2021deep" class="col-sm-8"> <div class="title">Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients</div> <div class="author"> Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Claudio P. Santiago, Soo K. Kim, Joanne T. Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sym_reg_2" class="col-sm-8"> <div class="title">Equation Discovery for Nonlinear System Identification</div> <div class="author"> Nikola Simidjievski, Ljupčo Todorovski, Juš Kocijan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sašo Džeroski' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Access</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ACCESS.2020.2972076" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ai_feyman" class="col-sm-8"> <div class="title">AI Feynman: A physics-inspired method for symbolic regression</div> <div class="author"> Silviu-Marian Udrescu and Max Tegmark </div> <div class="periodical"> <em>Science Advances</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1126/sciadv.aay2631" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Demirkaya2020ExploringTR" class="col-sm-8"> <div class="title">Exploring the Role of Loss Functions in Multiclass Classification</div> <div class="author"> Ahmet Demirkaya, Jiasi Chen, and Samet Oymak </div> <div class="periodical"> <em>2020 54th Annual Conference on Information Sciences and Systems (CISS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang_symbolic_2019" class="col-sm-8"> <div class="title">Symbolic regression in materials science</div> <div class="author"> Yiqun Wang, Nicholas Wagner, and James M. Rondinelli </div> <div class="periodical"> <em></em> 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1557/mrc.2019.85" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="bayesianIncreasing" class="col-sm-8"> <div class="title">Bayesian Statistics in Sociology: Past, Present, and Future</div> <div class="author"> Scott M. Lynch and Bryce Bartlett </div> <div class="periodical"> 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1146/annurev-soc-073018-022457" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Although Bayes’ theorem has been around for more than 250 years, widespread application of the Bayesian approach only began in statistics in 1990. By 2000, Bayesian statistics had made considerable headway into social science, but even now its direct use is rare in articles in top sociology journals, perhaps because of a lack of knowledge about the topic. In this review, we provide an overview of the key ideas and terminology of Bayesian statistics, and we discuss articles in the top journals that have used or developed Bayesian methods over the last decade. In this process, we elucidate some of the advantages of the Bayesian approach. We highlight that many sociologists are, in fact, using Bayesian methods, even if they do not realize it, because techniques deployed by popular software packages often involve Bayesian logic and/or computation. Finally, we conclude by briefly discussing the future of Bayesian statistics in sociology.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="bayesianPolynomial" class="col-sm-8"> <div class="title">A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions</div> <div class="author"> Eric Schulz, Maarten Speekenbrink, and Andreas Krause </div> <div class="periodical"> <em>Journal of Mathematical Psychology</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.jmp.2018.03.001" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration–exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Todorovski2017" class="col-sm-8"> <div class="title">Equation Discovery</div> <div class="author"> Ljupvco Todorovski </div> <div class="periodical"> 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1007/978-1-4899-7687-1_258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="pychology_bayes" class="col-sm-8"> <div class="title">A systematic review of Bayesian articles in psychology: The last 25 years.</div> <div class="author"> Rens Schoot, Sonja D. Winter, Oisı́n Ryan, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Mariëlle Zondervan-Zwijnenburg, Sarah Depaoli' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em></em> 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1037/met0000100" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Although the statistical tools most often used by researchers in the field of psychology over the last 25 years are based on frequentist statistics, it is often claimed that the alternative Bayesian approach to statistics is gaining in popularity. In the current article, we investigated this claim by performing the very first systematic review of Bayesian psychological articles published between 1990 and 2015 (n = 1,579). We aim to provide a thorough presentation of the role Bayesian statistics plays in psychology. This historical assessment allows us to identify trends and see how Bayesian methods have been integrated into psychological research in the context of different statistical frameworks (e.g., hypothesis testing, cognitive models, IRT, SEM, etc.). We also describe take-home messages and provide “big-picture” recommendations to the field as Bayesian statistics becomes more popular. Our review indicated that Bayesian statistics is used in a variety of contexts across subfields of psychology and related disciplines. There are many different reasons why one might choose to use Bayes (e.g., the use of priors, estimating otherwise intractable models, modeling uncertainty, etc.). We found in this review that the use of Bayes has increased and broadened in the sense that this methodology can be used in a flexible manner to tackle many different forms of questions. We hope this presentation opens the door for a larger discussion regarding the current state of Bayesian statistics, as well as future trends. (PsycInfo Database Record (c) 2020 APA, all rights reserved)</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="lucas_theis_lossy_2017" class="col-sm-8"> <div class="title">Lossy Image Compression with Compressive Autoencoders</div> <div class="author"> Lucas Theis, Wenzhe Shi, Andrew Cunningham, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ferenc Huszár' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.17863/cam.51995" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="kingma2017adam" class="col-sm-8"> <div class="title">Adam: A Method for Stochastic Optimization</div> <div class="author"> Diederik P. Kingma and Jimmy Ba </div> <div class="periodical"> 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sindyc" class="col-sm-8"> <div class="title">Sparse Identification of Nonlinear Dynamics with Control (SINDYc)</div> <div class="author"> Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz </div> <div class="periodical"> <em>IFAC-PapersOnLine</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ifacol.2016.10.249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract: Identifying governing equations from data is a critical step in the modeling and control of complex dynamical systems. Here, we investigate the data-driven identification of nonlinear dynamical systems with inputs and forcing using regression methods, including sparse regression. Specifically, we generalize the sparse identification of nonlinear dynamics (SINDY) algorithm to include external inputs and feedback control. This method is demonstrated on examples including the Lotka-Volterra predator-prey model and the Lorenz system with forcing and control. We also connect the present algorithm with the dynamic mode decomposition (DMD) and Koopman operator theory to provide a broader context.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gondara_medical_2016" class="col-sm-8"> <div class="title">Medical Image Denoising Using Convolutional Denoising Autoencoders</div> <div class="author"> Lovedeep Gondara </div> <div class="periodical"> <em></em> 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/icdmw.2016.0041" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sindy" class="col-sm-8"> <div class="title">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</div> <div class="author"> Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1073/pnas.1517384113" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="song2015deep" class="col-sm-8"> <div class="title">Deep Metric Learning via Lifted Structured Feature Embedding</div> <div class="author"> Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Silvio Savarese' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TrippletLoss" class="col-sm-8"> <div class="title">FaceNet: A unified embedding for face recognition and clustering</div> <div class="author"> Florian Schroff, Dmitry Kalenichenko, and James Philbin </div> <div class="periodical"> <em>In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/cvpr.2015.7298682" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="bayesian_data" class="col-sm-8"> <div class="title"></div> <div class="author"> Andrew Gelman, John Carlin, Hal Stern, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'David Dunson, Aki Vehtari, Donald Rubin' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jun 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1201/9780429258411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="kingma_auto-encoding_2013" class="col-sm-8"> <div class="title">Auto-Encoding Variational Bayes</div> <div class="author"> Diederik P. Kingma and Max Welling </div> <div class="periodical"> <em></em> Jun 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2009</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sym_reg_1" class="col-sm-8"> <div class="title">Distilling Free-Form Natural Laws from Experimental Data</div> <div class="author"> Michael Schmidt and Hod Lipson </div> <div class="periodical"> <em>Science</em>, Jun 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1126/science.1165893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2005</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="individual_diff" class="col-sm-8"> <div class="title">Modeling individual differences in cognition</div> <div class="author"> Michael D. Lee and Michael R. Webb </div> <div class="periodical"> <em>Psychonomic Bulletin &amp; Review</em>, Jun 2005 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.3758/BF03196751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="degree_of_belief" class="col-sm-8"> <div class="title">Degree-of-Belief and Degree-of-Support: Why Bayesians Need Both Notions</div> <div class="author"> James Hawthorne </div> <div class="periodical"> <em>Mind</em>, Jun 2005 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="contrastiveLoss" class="col-sm-8"> <div class="title">Learning a similarity metric discriminatively, with application to face verification</div> <div class="author"> Sumit Chopra, Raia Hadsell, and Yann. LeCun </div> <div class="periodical"> <em>In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</em>, Jun 2005 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/CVPR.2005.202" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2001</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="eq_discovery_theory" class="col-sm-8"> <div class="title">Theory Revision in Equation Discovery</div> <div class="author"> Ljupvco Todorovski and Savso Dvzeroski </div> <div class="periodical"> <em>In Discovery Science</em>, Jun 2001 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>State of the art equation discovery systems start the discovery process from scratch, rather than from an initial hypothesis in the space of equations. On the other hand, theory revision systems start from a given theory as an initial hypothesis and use new examples to improve its quality. Two quality criteria are usually used in theory revision systems. The first is the accuracy of the theory on new examples and the second is the minimality of change of the original theory. In this paper, we formulate the problem of theory revision in the context of equation discovery. Moreover, we propose a theory revision method suitable for use withth e equation discovery system Lagramge. The accuracy of the revised theory and the minimality of theory change are considered. The use of the method is illustrated on the problem of improving an existing equation based model of the net production of carbon in the Earth ecosystem. Experiments show that small changes in the model parameters and structure considerably improve the accuracy of the model.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="multilevel" class="col-sm-8"> <div class="title">Multilevel Modeling of Individual and Group Level Mediated Effects</div> <div class="author"> Jennifer L. Krull and David P. MacKinnon </div> <div class="periodical"> <em>Multivariate Behavioral Research</em>, Jun 2001 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1207/S15327906MBR3602_06" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2000</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="SR_symbolic_reg" class="col-sm-8"> <div class="title">Symbolic regression via genetic programming</div> <div class="author"> Douglas A. Augusto and Helio J.C. Barbosa </div> <div class="periodical"> <em>In Proceedings. Vol.1. Sixth Brazilian Symposium on Neural Networks</em>, Nov 2000 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/SBRN.2000.889734" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Presents an implementation of symbolic regression which is based on genetic programming (GP). Unfortunately, standard implementations of GP in compiled languages are not usually the most efficient ones. The present approach employs a simple representation for tree-like structures by making use of Read’s linear code, leading to more simplicity and better performance when compared with traditional GP implementations. Creation, crossover and mutation of individuals are formalized. An extension allowing for the creation of random coefficients is presented. The efficiency of the proposed implementation was confirmed in computational experiments which are summarized in the paper.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">1998</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="denison_bayesian_1998" class="col-sm-8"> <div class="title">Bayesian MARS</div> <div class="author"> D. G. T. Denison, B. K. Mallick, and A. F. M. Smith </div> <div class="periodical"> <em></em> Nov 1998 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1023/A:1008824606259" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">1991</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="mars" class="col-sm-8"> <div class="title">Multivariate Adaptive Regression Splines</div> <div class="author"> Jerome H. Friedman </div> <div class="periodical"> <em>The Annals of Statistics</em>, Nov 1991 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1214/aos/1176347963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">1955</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="chomsky" class="col-sm-8"> <div class="title">Logical Syntax and Semantics: Their Linguistic Relevance</div> <div class="author"> Noam Chomsky </div> <div class="periodical"> <em>Language</em>, Nov 1955 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lisa Artmann. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>