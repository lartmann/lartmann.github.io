---
---

@string{aps = {American Physical Society,}}

@article{wang2024multilingual,
  title={Multilingual E5 Text Embeddings: A Technical Report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}

@article{Rudin2018StopEB,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Cynthia Rudin},
  journal={Nature Machine Intelligence},
  year={2018},
  volume={1},
  pages={206 - 215},
}

@article{Arrieta2019ExplainableAI,
  title={Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  author={Alejandro Barredo Arrieta and Natalia D{\'i}az Rodr{\'i}guez and Javier Del Ser and Adrien Bennetot and Siham Tabik and A. Barbado and Salvador Garc{\'i}a and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  journal={Inf. Fusion},
  year={2019},
  volume={58},
  pages={82-115},
}

@article{Kung2022PerformanceOC,
  title={Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models},
  author={Tiffany H. Kung and Morgan Cheatham and Arielle Medenilla and Czarina Sillos and Lorie De Leon and Camille Elepa{\~n}o and Maria Madriaga and Rimel Aggabao and Giezel Diaz-Candido and James Maningo and Victor Tseng},
  journal={PLOS Digital Health},
  year={2022},
  volume={2},
}

@article{Gilson2023HowDC,
  title={How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment},
  author={Aidan Gilson and Conrad W Safranek and Thomas Huang and Vimig Socrates and Ling Chi and Richard Andrew Taylor and David Chartash},
  journal={JMIR Medical Education},
  year={2023},
  volume={9},
}

@article{Dwivedi2023OpinionP,
  title={Opinion Paper: "So what if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
  author={Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma L. Slade and Anand Jeyaraj and A. K. Kar and Abdullah Mohammed Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju K. Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence D. Brooks and Dimitrios Buhalis and Lemuria D. Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth Huw Davies and Robert M. Davison and Rahul De' and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavi{\'a}n and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paulette Camp Jones and Iris A. Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. T. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello M. Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobh{\'a}n O'Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil M.P. van der Aalst and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan D. Wright},
  journal={Int. J. Inf. Manag.},
  year={2023},
  volume={71},
  pages={102642},
}

@article{Singhal2022LargeLM,
  title={Large language models encode clinical knowledge},
  author={K. Singhal and Shekoofeh Azizi and Tao Tu and Said Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and Ajay Kumar Tanwani and Heather J. Cole-Lewis and Stephen J. Pfohl and P A Payne and Martin G. Seneviratne and Paul Gamble and Chris Kelly and Nathaneal Scharli and Aakanksha Chowdhery and P. A. Mansfield and Blaise Ag{\"u}era y Arcas and Dale R. Webster and Greg S. Corrado and Yossi Matias and Katherine Hui-Ling Chou and Juraj Gottweis and Nenad Toma{\vs}ev and Yun Liu and Alvin Rajkomar and Jo{\"e}lle K. Barral and Christopher Semturs and Alan Karthikesalingam and Vivek Natarajan},
  journal={Nature},
  year={2022},
  volume={620},
  pages={172 - 180},
}

@article{Yin2020DeepLF,
  title={Deep learning for brain disorder diagnosis based on fMRI images},
  author={Wutao Yin and Longhai Li and Fang-Xiang Wu},
  journal={Neurocomputing},
  year={2020},
  volume={469},
  pages={332-345},
}

@article{Ozcelik2024TheIA,
  title={The impact and future of artificial intelligence in medical genetics and molecular medicine: an ongoing revolution.},
  author={Firat Ozcelik and Mehmet Sait Dundar and Abdulbaki Yildirim and Gary Henehan and Oscar Vicente and Jos{\'e} Antonio S{\'a}nchez-Alc{\'a}zar and Nuriye Gokce and Duygu T. Yildirim and Nurdeniz Nalbant Bingol and Dijana Plaseska Karanfilska and Matteo Bertelli and Lejla Pojski{\'c} and Mehmet Ercan and Mikl{\'o}s Kellermayer and Izem Olcay Sahin and Ole K Greiner-Tollersrud and Busra Tan and Donald Martin and Robert Marks and Satya Prakash and Mustafa Yakubi and Tommaso Beccari and Ratnesh Lal and Sehime G. Temel and Isabelle Fournier and Mahmut Cerkez Ergoren and Adam Mechler and Michel Salzet and Michele Maffia and Dancho L. Danalev and Qun Sun and Lembit Nei and Daumantas Matulis and Dana Tăpăloagă and Andres Janecke and James Bown and Karla Santa Cruz and Iza Radecka and Celal Ozturk and Ozkan Ufuk Nalbantoglu and Sebnem Ozemri Sag and Kisung Ko and Reynir Arngr{\'i}msson and Isabel Belo and Hilal Akalın and Munis Dundar},
  journal={Functional \& integrative genomics},
  year={2024},
  volume={24 4},
  pages={
          138
        },
}

@article{Li2024ACR,
  title={A comprehensive review of artificial intelligence for pharmacology research},
  author={Bing Li and Kan Tan and Angelyn R. Lao and Haiying Wang and Huiru Zheng and Le Zhang},
  journal={Frontiers in Genetics},
  year={2024},
  volume={15},
}


@article{xumin_yu_group-aware_2021,
	title = {Group-aware Contrastive Regression for Action Quality Assessment},
	doi = {10.1109/iccv48922.2021.00782},
	abstract = {Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression ({CoRe}) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of {CoRe}, we conduct extensive experiments on three mainstream {AQA} datasets including {AQA}-7, {MTL}-{AQA} and {JIGSAWS}. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.},
	journaltitle = {IEEE International Conference on Computer Vision},
	author = {Xumin Yu and Yongming Rao and Wenliang Zhao and Jiwen Lu and Jie Zhou},
	year = {2021},
	doi = {10.1109/iccv48922.2021.00782}
}
	

@article{steffen_schneider_learnable_2022,
	title = {Learnable latent embeddings for joint behavioural and neural analysis},
	doi = {10.1038/s41586-023-06031-6},
	journaltitle = {Nature},
	author = {Steffen Schneider and Jin Hwa Lee and Mackenzie W. Mathis},
	year = {2022},
	doi = {10.1038/s41586-023-06031-6},
	pmcid = {10172131},
	pmid = {37138088}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-Encoding Variational Bayes},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {arXiv: Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2013}
}

@article{lucas_theis_lossy_2017,
	title = {Lossy Image Compression with Compressive Autoencoders},
	doi = {10.17863/cam.51995},
	abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with {JPEG} 2000 and outperforming recently proposed approaches based on {RNNs}. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
	journaltitle = {arXiv: Machine Learning},
	author = {Lucas Theis and Wenzhe Shi and Andrew Cunningham and Ferenc Huszár},
	year = {2017}
}

@article{gondara_medical_2016,
	title = {Medical Image Denoising Using Convolutional Denoising Autoencoders},
	abstract = {Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.},
	pages = {241--246},
	journaltitle = {2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
	author = {Lovedeep Gondara},
	year = {2016},
	doi = {10.1109/icdmw.2016.0041}
}


@article{wang_symbolic_2019,
	title = {Symbolic regression in materials science},
	volume = {9},
	issn = {2159-6859, 2159-6867},
	doi = {10.1557/mrc.2019.85},
	pages = {793--805},
	number = {3},
	journaltitle = {MRS Communications},
	shortjournal = {MRS Communications},
	author = {Wang, Yiqun and Wagner, Nicholas and Rondinelli, James M.},
	year = {2019},
	langid = {english}
}


@article{ai_feyman,
	abstract = {Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90\%.},
	author = {Silviu-Marian Udrescu and Max Tegmark},
	doi = {10.1126/sciadv.aay2631},

	journal = {Science Advances},
	number = {16},
	title = {AI Feynman: A physics-inspired method for symbolic regression},

	volume = {6},
	year = {2020},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/sciadv.aay2631},
	bdsk-url-2 = {https://doi.org/10.1126/sciadv.aay2631}
}


@INPROCEEDINGS{SR_symbolic_reg,
  author={Augusto, Douglas A. and Barbosa, Helio J.C.},
  booktitle={Proceedings. Vol.1. Sixth Brazilian Symposium on Neural Networks}, 
  title={Symbolic regression via genetic programming}, 
  year={2000},
  volume={},
  number={},
  pages={173-178},
  abstract={Presents an implementation of symbolic regression which is based on genetic programming (GP). Unfortunately, standard implementations of GP in compiled languages are not usually the most efficient ones. The present approach employs a simple representation for tree-like structures by making use of Read's linear code, leading to more simplicity and better performance when compared with traditional GP implementations. Creation, crossover and mutation of individuals are formalized. An extension allowing for the creation of random coefficients is presented. The efficiency of the proposed implementation was confirmed in computational experiments which are summarized in the paper.},
  keywords={Genetic programming;Linear code;Genetic mutations;Vectors;Predictive models;Genetic algorithms;Biological information theory},
  doi={10.1109/SBRN.2000.889734},
  ISSN={1522-4899},
  month={11}
}

@misc{petersen2021deep,
      title={Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients}, 
      author={Brenden K. Petersen and Mikel Landajuela and T. Nathan Mundhenk and Claudio P. Santiago and Soo K. Kim and Joanne T. Kim},
      year={2021},
      eprint={1912.04871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kingma2022autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{rajaratnam2015mcmcbased,
      title={MCMC-Based Inference in the Era of Big Data: A Fundamental Analysis of the Convergence Complexity of High-Dimensional Chains}, 
      author={Bala Rajaratnam and Doug Sparks},
      year={2015},
      eprint={1508.00947},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Demirkaya2020ExploringTR,
  title={Exploring the Role of Loss Functions in Multiclass Classification},
  author={Ahmet Demirkaya and Jiasi Chen and Samet Oymak},
  journal={2020 54th Annual Conference on Information Sciences and Systems (CISS)},
  year={2020},
  pages={1-5},
  url={https://api.semanticscholar.org/CorpusID:212414479}
}


@article{bayesianIncreasing,
	abstract = {Although Bayes' theorem has been around for more than 250 years, widespread application of the Bayesian approach only began in statistics in 1990. By 2000, Bayesian statistics had made considerable headway into social science, but even now its direct use is rare in articles in top sociology journals, perhaps because of a lack of knowledge about the topic. In this review, we provide an overview of the key ideas and terminology of Bayesian statistics, and we discuss articles in the top journals that have used or developed Bayesian methods over the last decade. In this process, we elucidate some of the advantages of the Bayesian approach. We highlight that many sociologists are, in fact, using Bayesian methods, even if they do not realize it, because techniques deployed by popular software packages often involve Bayesian logic and/or computation. Finally, we conclude by briefly discussing the future of Bayesian statistics in sociology.},
	author = {Lynch, Scott M. and Bartlett, Bryce},
	doi = {10.1146/annurev-soc-073018-022457},
	issn = {1545-2115},
	journal = {Annual Review of Sociology},
	keywords = {crisis in science},
	pages = {47-68},
	publisher = {Annual Reviews},
	title = {Bayesian Statistics in Sociology: Past, Present, and Future},
	type = {Journal Article},
	volume = {45},
	year = {2019},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev-soc-073018-022457},
	bdsk-url-2 = {https://doi.org/10.1146/annurev-soc-073018-022457}}

@article{multilevel,
author = {Jennifer L. Krull and David P. MacKinnon},
title = {Multilevel Modeling of Individual and Group Level Mediated Effects},
journal = {Multivariate Behavioral Research},
volume = {36},
number = {2},
pages = {249--277},
year = {2001},
publisher = {Routledge},
doi = {10.1207/S15327906MBR3602\_06},
}


@article{pychology_bayes,
	abstract = {Although the statistical tools most often used by researchers in the field of psychology over the last 25 years are based on frequentist statistics, it is often claimed that the alternative Bayesian approach to statistics is gaining in popularity. In the current article, we investigated this claim by performing the very first systematic review of Bayesian psychological articles published between 1990 and 2015 (n = 1,579). We aim to provide a thorough presentation of the role Bayesian statistics plays in psychology. This historical assessment allows us to identify trends and see how Bayesian methods have been integrated into psychological research in the context of different statistical frameworks (e.g., hypothesis testing, cognitive models, {IRT}, {SEM}, etc.). We also describe take-home messages and provide ``big-picture'' recommendations to the field as Bayesian statistics becomes more popular. Our review indicated that Bayesian statistics is used in a variety of contexts across subfields of psychology and related disciplines. There are many different reasons why one might choose to use Bayes (e.g., the use of priors, estimating otherwise intractable models, modeling uncertainty, etc.). We found in this review that the use of Bayes has increased and broadened in the sense that this methodology can be used in a flexible manner to tackle many different forms of questions. We hope this presentation opens the door for a larger discussion regarding the current state of Bayesian statistics, as well as future trends. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	author = {van de Schoot, Rens and Winter, Sonja D. and Ryan, Ois{\'\i}n and Zondervan-Zwijnenburg, Mari{\"e}lle and Depaoli, Sarah},
	year = {2017},
	doi = {10.1037/met0000100},
	issn = {1939-1463},
	journaltitle = {Psychological Methods},
	keywords = {*Methodology, *Models, *Psychology, *Statistical Probability, Analysis of Variance, Factor Analysis, Hypothesis Testing, Item Response Theory, Structural Equation Modeling},
	number = {2},
	pages = {217--239},
	title = {A systematic review of Bayesian articles in psychology: The last 25 years.},
	volume = {22},
	bdsk-url-1 = {https://doi.org/10.1037/met0000100}
}



@article{denison_bayesian_1998,
	author = {Denison, D. G. T. and Mallick, B. K. and Smith, A. F. M.},
	year = {1998},
	doi = {10.1023/A:1008824606259},
	issn = {1573-1375},
	journaltitle = {Statistics and Computing},
	number = {4},
	pages = {337--346},
	shortjournal = {Statistics and Computing},
	title = {Bayesian MARS},
	volume = {8},
	bdsk-url-1 = {https://doi.org/10.1023/A:1008824606259}
}



@article{mars,
	author = {Jerome H. Friedman},
	doi = {10.1214/aos/1176347963},
	journal = {The Annals of Statistics},
	keywords = {AID, CART, multivariable function approximation, multivariate smoothing, Nonparametric multiple regression, recursive partitioning, splines, statistical learning neural networks},
	number = {1},
	pages = {1 -- 67},
	publisher = {Institute of Mathematical Statistics},
	title = {Multivariate Adaptive Regression Splines},
	volume = {19},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176347963}
}


@article{bayesianPolynomial,
	abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration--exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
	author = {Eric Schulz and Maarten Speekenbrink and Andreas Krause},
	doi = {10.1016/j.jmp.2018.03.001},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	keywords = {Gaussian process regression, Active learning, Exploration--exploitation, Bandit problems},
	pages = {1-16},
	title = {A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions},
	volume = {85},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249617302158},
	bdsk-url-2 = {https://doi.org/10.1016/j.jmp.2018.03.001}
}

@INPROCEEDINGS{contrastiveLoss,
  author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Learning a similarity metric discriminatively, with application to face verification}, 
  year={2005},
  volume={1},
  pages={539-546 vol. 1},
  keywords={Character generation;Drives;Robustness;System testing;Spatial databases;Glass;Artificial neural networks;Support vector machines;Support vector machine classification;Face recognition},
  doi={10.1109/CVPR.2005.202}
}


@inproceedings{TrippletLoss,
   title={FaceNet: A unified embedding for face recognition and clustering},
   doi={10.1109/cvpr.2015.7298682},
   booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
   year={2015},
   month={6} 
}

@misc{song2015deep,
      title={Deep Metric Learning via Lifted Structured Feature Embedding}, 
      author={Hyun Oh Song and Yu Xiang and Stefanie Jegelka and Silvio Savarese},
      year={2015},
      eprint={1511.06452},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zha2023rankncontrast,
      title={Rank-N-Contrast: Learning Continuous Representations for Regression}, 
      author={Kaiwen Zha and Peng Cao and Jeany Son and Yuzhe Yang and Dina Katabi},
      year={2023},
      eprint={2210.01189},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{mds,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S.},
	year = {1952},
	doi = {10.1007/BF02288916},
	issn = {1860-0980},
	journaltitle = {Psychometrika},
	number = {4},
	pages = {401--419},
	shortjournal = {Psychometrika},
	title = {Multidimensional scaling: I. Theory and method},
	volume = {17},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}
}


@article{mds_gower,
	abstract = {This paper is concerned with the representation of a multivariate sample of size n as points P1, P2, {\ldots}, Pn in a Euclidean space. The interpretation of the distance Δ(Pi, Pj) between the ith and jth members of the sample is discussed for some commonly used types of analysis, including both Q and R techniques. When all the distances between n points are known a method is derived which finds their co-ordinates referred to principal axes. A set of necessary and sufficient conditions for a solution to exist in real Euclidean sapce is found. Q and R techniques are defined as being dual to one another when they both lead to a set of n points with the same inter-point distances. Pairs of dual techniques are derived. In factor analysis the distances between points whose co-ordinrates are the estimated factor scores can be interpreted as D2 with a singular dispersion matrix.},
	author = {Gower, J. C.},
	year = {1966},
	doi = {10.1093/biomet/53.3-4.325},
	issn = {0006-3444},
	journaltitle = {Biometrika},
	number = {3},
	pages = {325--338},
	title = {Some distance properties of latent root and vector methods used in multivariate analysis},
	volume = {53},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/53.3-4.325}
}


@inproceedings{eq_discovery_theory,
	abstract = {State of the art equation discovery systems start the discovery process from scratch, rather than from an initial hypothesis in the space of equations. On the other hand, theory revision systems start from a given theory as an initial hypothesis and use new examples to improve its quality. Two quality criteria are usually used in theory revision systems. The first is the accuracy of the theory on new examples and the second is the minimality of change of the original theory. In this paper, we formulate the problem of theory revision in the context of equation discovery. Moreover, we propose a theory revision method suitable for use withth e equation discovery system Lagramge. The accuracy of the revised theory and the minimality of theory change are considered. The use of the method is illustrated on the problem of improving an existing equation based model of the net production of carbon in the Earth ecosystem. Experiments show that small changes in the model parameters and structure considerably improve the accuracy of the model.},
	address = {Berlin, Heidelberg},
	author = {Todorovski, Ljupvco and Dvzeroski, Savso},
	booktitle = {Discovery Science},
	editor = {Jantke, Klaus P. and Shinohara, Ayumi},
	isbn = {978-3-540-45650-6},
	pages = {389--400},
	publisher = {Springer Berlin Heidelberg},
	title = {Theory Revision in Equation Discovery},
	year = {2001}
}


@article{sym_reg_1,
	author = {Michael Schmidt and Hod Lipson},
	doi = {10.1126/science.1165893},
	journal = {Science},
	number = {5923},
	pages = {81-85},
	title = {Distilling Free-Form Natural Laws from Experimental Data},
	volume = {324},
	year = {2009}
}


@inbook{Todorovski2017,
	address = {Boston, MA},
	author = {Todorovski, Ljupvco},
	booktitle = {Encyclopedia of Machine Learning and Data Mining},
	doi = {10.1007/978-1-4899-7687-1_258},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	isbn = {978-1-4899-7687-1},
	pages = {410--414},
	publisher = {Springer US},
	title = {Equation Discovery},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4899-7687-1_258}
}



@article{sym_reg_2,
  author={Simidjievski, Nikola and Todorovski, Ljupčo and Kocijan, Juš and Džeroski, Sašo},
  journal={IEEE Access}, 
  title={Equation Discovery for Nonlinear System Identification}, 
  year={2020},
  volume={8},
  number={},
  pages={29930-29943},
  keywords={Mathematical model;Computational modeling;Data models;Integrated circuit modeling;Biological system modeling;Nonlinear systems;Context modeling;Machine learning;nonlinear system identification;equation discovery;process-based modeling;computational scientific discovery;knowledge-based identification},
  doi={10.1109/ACCESS.2020.2972076}
}


@article{individual_diff,
	author = {Lee, Michael D. and Webb, Michael R.},
	doi = {10.3758/BF03196751},
	id = {Lee2005},
	journal = {Psychonomic Bulletin \& Review},
	number = {4},
	pages = {605--621},
	title = {Modeling individual differences in cognition},
	volume = {12},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.3758/BF03196751}
}

@article{degree_of_belief,
 ISSN = {00264423, 14602113},
 URL = {http://www.jstor.org/stable/3489107},
 author = {James Hawthorne},
 journal = {Mind},
 number = {454},
 pages = {277--320},
 publisher = {[Oxford University Press, Mind Association]},
 title = {Degree-of-Belief and Degree-of-Support: Why Bayesians Need Both Notions},
 volume = {114},
 year = {2005}
}

@book{bayesian_data,
    author = {Andrew Gelman and John Carlin and Hal Stern and David Dunson and Aki Vehtari and Donald Rubin},
    address = {Boca Raton, FL},
    publisher = {Taylor \& Francis Group},
    year = {2014},
    edition = {3rd},
    chapter = {Probability as a measure of uncertainty},
    doi = {10.1201/9780429258411},
    url = {https://doi.org/10.1201/9780429258411},
    pages = {11-13}
}

@manual{pytorch_relu,
  title = {torch.nn.ReLU},
  author = {PyTorch Contributors},
  year = {2023},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html},
  note = {Accessed: 2024-05-15}
}


@article{sindy,
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	author = {Steven L. Brunton and Joshua L. Proctor and J. Nathan Kutz},
	doi = {10.1073/pnas.1517384113},
	journal = {Proceedings of the National Academy of Sciences},
	number = {15},
	pages = {3932-3937},
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	year = {2016}
}


@article{bayesian_sindy,
	abstract = { We propose a probabilistic model discovery method for identifying ordinary differential equations governing the dynamics of observed multivariate data. Our method is based on the sparse identification of nonlinear dynamics (SINDy) framework, where models are expressed as sparse linear combinations of pre-specified candidate functions. Promoting parsimony through sparsity leads to interpretable models that generalize to unknown data. Instead of targeting point estimates of the SINDy coefficients, we estimate these coefficients via sparse Bayesian inference. The resulting method, uncertainty quantification SINDy (UQ-SINDy), quantifies not only the uncertainty in the values of the SINDy coefficients due to observation errors and limited data, but also the probability of inclusion of each candidate function in the linear combination. UQ-SINDy promotes robustness against observation noise and limited data, interpretability (in terms of model selection and inclusion probabilities) and generalization capacity for out-of-sample forecast. Sparse inference for UQ-SINDy employs Markov chain Monte Carlo, and we explore two sparsifying priors: the spike and slab prior, and the regularized horseshoe prior. UQ-SINDy is shown to discover accurate models in the presence of noise and with orders-of-magnitude less data than current model discovery methods, thus providing a transformative method for real-world applications which have limited data. },
	author = {Hirsh, Seth M. and Barajas-Solano, David A. and Kutz, J. Nathan},
	doi = {10.1098/rsos.211823},
	journal = {Royal Society Open Science},
	number = {2},
	pages = {211823},
	title = {Sparsifying priors for Bayesian uncertainty quantification in model discovery},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rsos.211823},
	bdsk-url-2 = {https://doi.org/10.1098/rsos.211823}
}


@article{sindyc,
	abstract = {Abstract: Identifying governing equations from data is a critical step in the modeling and control of complex dynamical systems. Here, we investigate the data-driven identification of nonlinear dynamical systems with inputs and forcing using regression methods, including sparse regression. Specifically, we generalize the sparse identification of nonlinear dynamics (SINDY) algorithm to include external inputs and feedback control. This method is demonstrated on examples including the Lotka-Volterra predator-prey model and the Lorenz system with forcing and control. We also connect the present algorithm with the dynamic mode decomposition (DMD) and Koopman operator theory to provide a broader context.},
	author = {Steven L. Brunton and Joshua L. Proctor and J. Nathan Kutz},
	doi = {10.1016/j.ifacol.2016.10.249},
	issn = {2405-8963},
	journal = {IFAC-PapersOnLine},
	keywords = {Dynamical systems, control, system identification, sparse regression},
	number = {18},
	pages = {710-715},
	title = {Sparse Identification of Nonlinear Dynamics with Control (SINDYc)},
	url ={https://www.sciencedirect.com/science/article/pii/S2405896316318298},
	volume = {49},
	year = {2016},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2405896316318298},
	bdsk-url-2 = {https://doi.org/10.1016/j.ifacol.2016.10.249}
}

@article{chomsky,
     ISSN = {00978507, 15350665},
     URL = {http://www.jstor.org/stable/410891},
     author = {Noam Chomsky},
     journal = {Language},
     number = {1},
     pages = {36--45},
     publisher = {Linguistic Society of America},
     title = {Logical Syntax and Semantics: Their Linguistic Relevance},
     volume = {31},
     year = {1955}
}


@article{generative_vae,
	abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
	author = {GM Harshvardhan and Mahendra Kumar Gourisaria and Manjusha Pandey and Siddharth Swarup Rautaray},
	doi = {10.1016/j.cosrev.2020.100285},
	issn = {1574-0137},
	journal = {Computer Science Review},
	keywords = {Generative models, Machine learning, Deep learning, Neural networks, Bayesian inference},
	pages = {100285},
	title = {A comprehensive survey and analysis of generative models in machine learning},
	volume = {38},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
	bdsk-url-2 = {https://doi.org/10.1016/j.cosrev.2020.100285}
}

@article{kl,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2236703},
 author = {S. Kullback and R. A. Leibler},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {79--86},
 publisher = {Institute of Mathematical Statistics},
 title = {On Information and Sufficiency},
 volume = {22},
 year = {1951}
}

@article{Blei_2017,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   ISSN={1537-274X},
   DOI={10.1080/01621459.2017.1285773},
   number={518},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
   year={2017},
   month={4}, 
pages={859–877} }

@misc{odaibo2019tutorial,
      title={Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function}, 
      author={Stephen Odaibo},
      year={2019},
      eprint={1907.08956},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}